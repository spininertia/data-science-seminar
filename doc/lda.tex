LDA\cite{blei2003latent} is a natural extension to pLSA. There are several obvious drawbacks of pLSA. First, in pLSA, each document has its own distribution over topics, thus the number of parameters of pLSA grows linearly with the number of documents, we need a generative model to solve the disaster of parameter explosion. Secondly, The model of pLSA only concerns about \emph{training} documents, which means that pLSA is difficult to do inference about unseen documents. These restrictions deistically limits the usage of pLSA. 

Based on the representation theorem due to Finetti\cite{de1970theory}, LDA puts the generation of document and topic in a probability setting, which leads to a fully probabilistic model. 

\subsection{Graphical Model}
Graph Model is a powerful tool in multivariate probability distribution. It's designed to help design efficient learning and inference algorithms. The key insight of graph modeling is that in many real world probability distributions, the joint probability distributions can be factored into several smaller local probability distribution. The advantage of this factorization leads to much smaller model complexity. It turns out that this convenient property is closely related to the conditional independence between the random variables. Thus in graph modeling, it's relatively easy to spot conditional independent relationships between variables by just observing the graph structure of the graph model. Actually this is the major reason why people favor it. Next we'll introduce two major types of graph modeling techniques. 

\subsubsection{Undirected Model}
The notation we use follows \cite{}. We considers probability distributions over two sets of random variables, $V=X \cup Y$, where X is the set of \emph{input variables} that are observed, and Y is the set of \emph{output variables} that we want to predict. The domain of each variable's outcome can be continuous or discrete. An arbitrary assignment to X is denoted by a vector {\bf x}. Given a variable $s \in X$, the notation $x_{s}$ denotes the value assigned to s by {\bf x}. ${\bf 1}_{x=x'}$ denotes an indicator function of x which takes value 1 when $x=x'$ and 0 otherwise. 

We assume the probability distribution can be represented by a product of \emph{factors} of the form $\Psi_{a}(x_{a},y_{a})$,where each factor has scope $a \subset V$. The gain for this representation is that each subset contains a much smaller number of variables. 

An undirected model is a family of probability distributions that factorize according to given collections of scopes. Given a collections of subsets $\mathcal{F}=a \subset V$, an \emph{undirected graphical model} is defined as the set of all distributions that can be written as :

\begin{equation}
p(x,y) = \frac{1}{Z} \prod_{a \in \mathcal{F}} \Psi_{a}(x_{a},y_{a})
\end{equation}

for any choice of \emph{local function} $F = {\Psi_{a}}$. The Z is just a normalization constant to make the probability sum to 1. 

\subsubsection{Directed Model}
Different from the \emph{local function} in undirected graph models that do not have a direct probabilistic interpretation, a \emph{directed graphical model} describes how a distribution factorizes into local conditional probability distributions. Let $G=(V,E)$ be a directed acyclic graph, in which $\pi_{v}$ are the parents of v in G. A directed graph model is a family of distributions that factorize as:
\begin{equation}
p(y,x)=\prod_{v \in V} p(y_{v}|y_{\pi(v)})
\end{equation}

As we can observe from the formula, a directed graph model can be seen as a kind of factor graph. The \emph{local function} is automatically normalized, thus Z=1. It's used in generative models extensively. 

\subsection{Inference}
Although Latent Dirichlet allocation is a rather simple model, exact inference is generally intractable. The solution to this is to use approximate inference algorithms, such as mean-field variational expectation maximization \cite{blei2003latent}, expectation propagation\cite{minka2002expectation} and Gibbs sampling \cite{griffiths2002gibbs}. In the following, we briefly introduce two of them, Gibbs Sampling and variational inference.

\subsubsection{Gibbs Sampling}
Gibbs Sampling method is a special case of Markov-chain Monte Carlo(MCMC) simulation, it often yields relatively simple algorithms for approximate inference in high dimensional models such as LDA. MCMC methods can emulate high-dimensionality probability distributions $p(\vec{x})$ by the stationary behavior of a Markov chain. This means that one sample is generated for each transit in the chain after a stationary state of the chain has been reached. Gibbs sampling is a special case of MCMC where the dimensions $x_i$ of the distribution are sampled alternatively one at a time, conditioned on the values of all other dimensions. The algorithm can be roughly described as follows:
\begin{enumerate}
\item choose dimension i 
\item sample $x_i$ from $p(x_i | \vec{x}_{\neg i})$
\end{enumerate}
To build the gibbs sampler, the conditional probability $p(x_i | \vec{x}_{\neg i})$ must be found. For models that contain hidden variables $\vec{z}$, the gibbs sampler can be represented as:
\begin{equation}
p(z_i | \vec{z}_{\neg i}, \vec{x}) = \frac{p(\vec{z}, \vec{x})}{p(\vec{z}_{\neg i}, \vec{x})} =  \frac{p(\vec{z}, \vec{x})}{\int_Z p(\vec{z}, \vec{x}) d_{z_i}}
\end{equation} 
where the integral changes to a sum for discrete variables. With a sufficient number of samples $\vec{z}_r^{\sim}$ where $r \in [1, R]$, the latent-cariable posterior can be approximated using formula:
\begin{equation} 
p(\vec{z}|\vec{x}) \approx \frac{1}{R}\sum_{r=1}^R\delta(\vec{z} - \vec{z}_r^{\sim}),
\end{equation}
with the Kronecker delta $\delta(\vec(u)) = \{1 if \vec(u) = 0; 0, otherwise\}$.

Applying gibbs sampling algorithm to the inference of LDA yields:
\begin{equation}
\phi_{k, t} = \frac{n_k^{(t)} + \beta_t}{\sum_{t=1}^Vn_k^{(t)} + \beta_t},
\end{equation}

\begin{equation}
\theta_{m,k} = \frac{n_m^{(k)} + \alpha_k}{\sum_{k=1}^K n_m^{(k)} + \alpha_k},
\end{equation}
where $n_k^{(t)}$ represent the count for term t that occurs in topic t, and $n_m^{(k)}$ represents the count of terms in document m that belongs to topic k.

The Gibbs sampling algorithm runs over the three periods: initialization, burn-in and sampling. However, to determine the required lengths of the burn-in is one of the drawbacks with MCMC approaches. There exists several criteria to check that the Markov chain has converged \cite{liu2008monte}.

\subsubsection{Variational Inference}
The basic idea of convexity-based variational inference is to make use of Jensen’s inequality to obtain an adjustable lower bound on the log likelihood. Essentially, one considers a family of lower bounds, indexed by a set of variational parameters. The variational parameters are chosen by an optimization procedure that attempts to find the tightest possible lower bound.

A simple way to obtain a tractable family of lower bounds is to consider simple modifications of the original graphical model in which some of the edges and nodes are removed. The intractable essence for the inference of original model is caused by the problematic coupling between $\theta$ and $\beta$. By dropping the edges with $w$ and the $w$ nodes, we obtain a family of distributions on the latent variables. This family is characterized by the following variational distribution
\begin{equation}
q(\theta, z|\gamma, \Phi) = q(\theta|\gamma)\Pi_{n=1}^Nq(z_n|\Phi_n)
\end{equation}
where the dirichlet parameter $\gamma$ and the multinomial parameters $\Phi$ are the free variational parameters. Having this simplified parameters, we try to find the tight lower bound on the likelihood function:
\begin{equation}
(\gamma^*, \Phi^*) = argmin_{\gamma, \Phi}D(q(\theta|\gamma)\Pi_{n=1}^Nq(z_n|\Phi_n) || p(\theta, z| w, \alpha, \beta)).
\end{equation}
This is just seeking the variational parameters that minimize the KL-divergence between the variational distribution and the original posterior function. We can solve this optimization problem by iteratively doing dirichlet update(details in paper \cite{blei2003latent}). Having the optimal variation parameter, we can do tractable approximate inference of LDA using this simplified distribution. 

\subsection{Learning}
In the inference step, we assume the corpus-wise parameters $\alpha$, $\beta$ are known. Typically, in LDA symmetric Dirichlet priors are used, which means that the a priori assumption of the model is that all topics have the same chance of being assigned to a document and all words (frequent and infrequent ones) have the same chance of being assigned to a topic. 
But there are methods that utilize the count in Gibbs Sampling or learned variational distribution in Variational Inference to estimate the Dirichlet parameters $\vec{\alpha}$. We won't talk about the detail of the techniques, for reference, see paper\cite{heinrich2005parameter}\cite{blei2003latent}


\subsection{Extension}
A natural extension to LDA is the ability to model the correlation between topics. \cite{lafferty2005correlated} presented a way to encode the correlation between topics by replacing Dirichlet distribution with a logistic normal distribution. \cite{griffiths2004hierarchical} proposed another approach to this goal, which is called hierarchical LDA(hLDA), by join the topics in a hierarchy using Chinese restaurant process. In addition to model the topic relationship, several work extend the application beyond document analysis. \cite{shu2009latent} extends LDA to handle heterogeneous data, in other words, there are two type of information in a document, words and names. 

Another promising direction is develop nonparametric version of LDA. Hierarchical Dirichlet process mixture model \cite{teh2006hierarchical} can learn the number of topics from data. Nested Chinese Restaurant Process \cite{griffiths2004hierarchical} allows topics to be structured in a hierarchical structure which is also learnt from data. 
