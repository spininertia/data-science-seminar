LDA\cite{blei2003latent} is a natural extension to pLSA. There are several obvious drawbacks of pLSA. First, in pLSA, each document has its own distribution over topics, thus the number of parameters of pLSA grows linearly with the number of documents, we need a generative model to solve the disaster of parameter explosion. Secondly, The model of pLSA only concerns about \emph{training} documents, which means that pLSA is difficult to do inference about unseen documents. These restrictions deistically limits the usage of pLSA. 

Based on the representation theorem due to Finetti\cite{de1970theory}, LDA puts the generation of document and topic in a probability setting, which leads to a fully probabilistic model. 

\subsection{Graphical Model}
Intro to graph model. \cite{jordan1998introduction}

\subsection{Learning}
Parameter estimation for LDA. \cite{heinrich2005parameter}

\subsection{Inference}
There are many approximate algorithms of inference. We will survey some simple and effective ones.

\subsubsection{Gibbs Sampling}
\cite{casella1992explaining}

\subsubsection{Variational Method}