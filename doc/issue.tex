\subsection{Parallelism}
With current rapid development in parallel computing, we consider implement LDA under such setting is a interesting topic. Meanwhile, there are some challenge problems in achieving good parallelism. First, Gibbs sampling is a high sequential procedure. Each step in Gibbs sampling depends on previous instructions and a set of global counters, which is not a problem in single machine setting since all parameters are in memory. In this section, we will introduce several prevailing programming models and experience gained from implementing LDA in each platform. 

\subsubsection{Programming Model}
The main motivation behind parallel computing is to leverage current multi-processor architecture and grid computing technology. There exists several popular \emph{programming models} already. The most used is \emph{Thread}, and its standard open implementation is POSIX Threads\cite{butenhof1997programming}. Message Passing can be viewed as a variant of thread in distributed setting, tasks exchange data through communications by sending and receiving messages. The most used implementation is OpenMPI\cite{lusk2009mpi}, which is the "de facto" industry standard. In recent yeas, a novel programming model named \emph{MapReduce}\cite{dean2008mapreduce} gets more and more attention by researchers. The main idea is that there are two basic routines, namely \emph{Map} and \emph{Reduce}. \emph{Map} applies a function to a large collection of data and returns a list of key-value pair. \emph{Reduce} then collects the results and merge values by keys. Both \emph{Map} and \emph{Reduce} can run in parallel. 

\subsubsection{Parallel LDA}
Even though LDA is still an intuitive and simple model, exact inference is considered intractable. A general approach is Gibbs sampling. While the problem with Gibbs Sampling is that it has to go through the whole corpus and assign a topic on each word. This embedded sequential nature limits efficiency. When dealing with larger collection of documents, a new design of algorithm is always needed. Several work has explored the idea of implementing Gibbs sampling across multiple machines. \cite{smyth2008asynchronous} proposed an asynchronous version of Gibbs sampling. And \cite{newman2007distributed} demonstrated two synchronous approaches, namely AD-LDA and HDLDA. The core idea of AD-LDA is to partition dataset into different blocks. Each processor is assigned its own subset of data and related parameters. After some period of time(a single pass through the data on each processor), processors communicate to each to synchronize a global set of counts. And HD-LDA is a more principle way to tackle the parallel issue. It encode parallel computation directly into the model, that we can view HD-LDA as a mixture model with P LDA mixture components. \cite{wang2009plda} ported AD-LDA to both MPI and MapReduce. They applied both implementation to Wikipedia dataset. Preliminary results showed that both MPI-LDA and MapReduce-LDA enjoyed linear speedup when the number of machines is under 100. 
