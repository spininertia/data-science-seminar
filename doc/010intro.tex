Topic Model has been a successful tool for managing large text data set in different fields\cite{wallach2006topic}. For example, it's been applied to index latent \emph{term} in Information Retrieval\cite{deerwester1990indexing}, which proves its power to mining hidden states. Different models and algorithms are have been proposed during its development, for example, \cite{dumais1995latent}, \cite{blei2003latent}and\cite{hofmann1999probabilistic}. 

The report will discuss the overall development of topic model, and several breakthrough in its history, namely Latent Semantic Analysis, Probabilitics Latent Semantic Analysis, Latent Dirichilet Allocation. From machine learning's perspectie, the main difference of LDA from its predecessors is that it is no longer a term-document model, instead it totally embrace probablity setting. Thus we can apply powerful statistical tools to do inference and learning of LDA, mainly graphical model. We will introduce general framework of graphical model and its common techniques, like Variational Inference, Gibbs Sampling, Variabtion EM, etc.

Despite the maintream models, we will also cover some open issues of Topic Models(mainlt about LDA). Parallalism is a good way to speed algoriths, and GPU is a popular vehicle to conduct experiments. But it's extremely difficult to implement Gibbs Sampling on GPU. We will mentions some latest progress on this topic. Also some other popular variants of LDA needs to be mentioned for completeness. 

